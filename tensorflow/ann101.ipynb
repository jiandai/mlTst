{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network 101\n",
    "## With Iris dataset and tensorflow\n",
    "\n",
    "##### Jian Dai, PhD (daij12@gene.com)\n",
    "\n",
    "### Goal\n",
    "\n",
    "- In genneral, understand the foundation of artificial neural network including model architecture, and training/optimization using Iris dataset as example and tensorflow as toolkit\n",
    "- More specifically, **Exercise 1** compute gradient and weight update with single batch, and compare with the result of calling tensorflow for multinomial model to fully understand *gradient descent*\n",
    "- **Exercise 2** compute gradient and weight update with single batch, and compare with the result of calling tensorflow for 1-hidden-layer model to fully understand *backpropagation*\n",
    "\n",
    "### Plan\n",
    "\n",
    "- Iris datase, Tensorflow\n",
    "- Multinomial regression (aka softmax regression)\n",
    "    - Gradient descent\n",
    "- 1-Hidden layer feedback forward neural network\n",
    "    - Backprogagation\n",
    "    - Tensor computation\n",
    "- Some to-do for next steps:\n",
    "    - Gradient descent with momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n",
    "\n",
    "In the following we will use Iris dataset provided by the [tensorflow tutorial](https://en.wikipedia.org/wiki/Iris_flower_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2           3           4\n",
      "count  120.000000  120.000000  120.000000  120.000000  120.000000\n",
      "mean     5.845000    3.065000    3.739167    1.196667    1.000000\n",
      "std      0.868578    0.427156    1.822100    0.782039    0.840168\n",
      "min      4.400000    2.000000    1.000000    0.100000    0.000000\n",
      "25%      5.075000    2.800000    1.500000    0.300000    0.000000\n",
      "50%      5.800000    3.000000    4.400000    1.300000    1.000000\n",
      "75%      6.425000    3.300000    5.100000    1.800000    2.000000\n",
      "max      7.900000    4.400000    6.900000    2.500000    2.000000\n"
     ]
    }
   ],
   "source": [
    "# Assume these two csv's 'iris_training.csv', 'iris_test.csv' are in the working dir\n",
    "# Otherwise download from\n",
    "# http://download.tensorflow.org/data/iris_training.csv\n",
    "# http://download.tensorflow.org/data/iris_test.csv\n",
    "\n",
    "import pandas as pd\n",
    "training_df = pd.read_csv('iris_training.csv',skiprows=1,header=None)\n",
    "training_features = training_df.iloc[:,0:4].values\n",
    "training_labels = training_df.iloc[:,4].values\n",
    "test_df = pd.read_csv('iris_test.csv',skiprows=1,header=None)\n",
    "test_features = test_df.iloc[:,0:4].values\n",
    "test_labels = test_df.iloc[:,4].values\n",
    "print(training_df.describe())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For R users, refer the following line for reading the data\n",
    "<pre>\n",
    "read.csv('iris_training.csv',skip=1,header=FALSE)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# One-hot coding for class labels\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet, dense_to_one_hot\n",
    "training_labels_1h = dense_to_one_hot(training_labels,3)\n",
    "training_dataset = DataSet(training_features, training_labels_1h,reshape=False)\n",
    "test_labels_1h = dense_to_one_hot(test_labels,3)\n",
    "test_dataset = DataSet(test_features, test_labels_1h,reshape=False)\n",
    "\n",
    "print(training_dataset.num_examples)\n",
    "print(test_dataset.num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow\n",
    "https://www.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial regression model\n",
    "\n",
    "Here is a brief setup for a multinomial regression (aka softmax regression)\n",
    "\n",
    "Introduce the multinomial logit as\n",
    "\n",
    "$$ \\eta_k = x_\\mu W_k^\\mu $$\n",
    "\n",
    "where $x^i_\\mu$ the input with an implicit sample index $i$ and the feature index $\\mu$ including the *bias* term. For Iris data, $k$ indexes 3 classes,  $\\mu$ indexes 4 predictors and the bias, and $i$ indexes the batched sample. There is a summation over $\\mu$ in the above expression understood. We keep a freedom to specify the summary on a pair of indices implicitly or explicitly. If the summation is implied, the convention is known as *[Einstein summation](http://mathworld.wolfram.com/EinsteinSummation.html)* which is commonly used in classical tensor analysis in mathematics and physics.\n",
    "\n",
    "With logit, the conditional probability given data is \n",
    "\n",
    "$$ p_k = \\frac{e^{\\eta_k}}{Z} $$\n",
    "\n",
    "where $Z = \\sum_k e^{\\eta_k}$ is known as *partition function*. Or if we specify the implicit sample index, $p_k^i = e^{\\eta_k^i} / Z^i$. Please be cautious not to confuse superindex and power.\n",
    "\n",
    "The loss function is defined as\n",
    "\n",
    "\\begin{equation} l = - \\Bigg \\langle \\sum_{k=1}^3 y_k \\log p_k \\Bigg \\rangle \\end{equation}\n",
    "\n",
    "where $y$ the observed class with an implicit sample index $i$. In this formalism, $y$ is expressed by *one-hot encoding*.\n",
    "\n",
    "Here the average operation $\\Big\\langle\\ldots\\Big\\rangle$ can be specified differently in different context. For example,\n",
    "\n",
    "$$\\Bigg\\langle\\ldots\\Bigg\\rangle = \\int dP(x,y)$$\n",
    "\n",
    "if the joint distribution is known, or\n",
    "\n",
    "$$\\Bigg\\langle\\ldots\\Bigg\\rangle = \\sum_i$$\n",
    "\n",
    "for batch or mini-batch empirical loss, or just a single case in [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). Also one can choose a normalization in the average. Commonly used normalization includes 2 for Deviance https://en.wikipedia.org/wiki/Deviance_(statistics) in classical statistical analysis, $1/N$ for categorical [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) in ML. In our case, we multiple a factor $\\log 3$ such the *uninformative* prediction gives raise to $l=1$. To restate for our practice and for Iris data\n",
    "\n",
    "\\begin{equation} l = - {1\\over {N\\log 3}} \\sum_i^N \\sum_{k=1}^3 y^i_k \\log p^i_k  \\end{equation}\n",
    "\n",
    "where $N$ is the sample size in a mini batch.\n",
    "\n",
    "Note the constraints $\\sum_k p_k = 1$ and $\\sum_k y_k=1$. And the loss can be transformed to a form which is easier to derive gradient\n",
    "\n",
    "\\begin{equation} l = - {1\\over {N\\log 3}} \\sum_i^N \\big(\\sum_{k=1}^3 y^i_k \\eta^i_k  - \\log Z^i\\big)  \\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow approach to build a Multinomial regression model for Iris data\n",
    "# Predictors\n",
    "x = tf.placeholder(tf.float32,[None,4])\n",
    "# Weight\n",
    "W = tf.Variable(tf.random_normal([4,3]),tf.float32)\n",
    "# bias\n",
    "b = tf.Variable(tf.zeros([3]),tf.float32)\n",
    "# logits\n",
    "logits = tf.add(tf.matmul(x,W),b)\n",
    "# convert to probability\n",
    "prob = tf.nn.softmax(logits)\n",
    "# Actual class\n",
    "y = tf.placeholder(tf.float32,[None,3])\n",
    "\n",
    "# Define the loss\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(prob),reduction_indices=[1]) / tf.log(3.)\n",
    "# normalize by log(3) just so the ``uninformative'' prob gives cross-entropy 1.\n",
    "loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = .05\n",
    "batch_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the optimization method\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "fit = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derive the gradient of loss function with respect to the weight\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial W^\\mu_k} = -\\Big \\langle x_\\mu (y_k - p_k) \\Big \\rangle $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient of loss /w respect to W\n",
    "grad_W = -tf.matmul(tf.transpose(x),(y-prob))/tf.log(3.)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient of loss /w respect to bias\n",
    "grad_b = -tf.matmul(tf.ones([1,batch_size]),(y-prob))/tf.log(3.)/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gradient descent optimization](https://en.wikipedia.org/wiki/Gradient_descent) is to solve the following *auxilary 1st order ODE*\n",
    "$$\\dot{W^\\mu_k} = - \\frac{\\partial l}{\\partial W^\\mu_k} $$\n",
    "\n",
    "where we introduce a pseudo time $\\tau$ and $\\dot{f} = df/d\\tau$.\n",
    "\n",
    "With [*forward Euler method*](https://en.wikipedia.org/wiki/Euler_method)\n",
    "\n",
    "$$ W^\\mu_k(\\tau_{n+1}) = W^\\mu_k(\\tau_n) + \\delta \\tau \\Big \\langle x_\\mu (y_k - p_k) \\Big \\rangle $$\n",
    "\n",
    "which is the standard *Gradient Descent* formalism, where $\\delta\\tau$ is referred as *learning rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sess = tf.Session()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Record the weight before a single batch\n",
    "W0,b0 = sess.run([W,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get next mini batch\n",
    "btch_x,btch_y = training_dataset.next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the gradient\n",
    "delta_W, delta_b = sess.run([-lr*grad_W, -lr*grad_b],{x:btch_x,y:btch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow computed model update\n",
    "sess.run(fit,{x:btch_x,y:btch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Updated weight per a single batch\n",
    "W1,b1 = sess.run([W,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### The following two Jupyter Notebook cells compare the tensorflow weight update and our computed weight update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  2.12073326e-04,  -1.80065632e-04,  -3.19480896e-05],\n",
       "        [  1.66893005e-04,  -1.18255615e-04,  -4.85926867e-05],\n",
       "        [ -9.17911530e-06,  -7.83205032e-05,   8.74996185e-05],\n",
       "        [ -2.36630440e-05,  -2.38418579e-05,   4.75645065e-05]], dtype=float32),\n",
       " array([[  2.12033963e-04,  -1.80060204e-04,  -3.19737555e-05],\n",
       "        [  1.66803147e-04,  -1.18209478e-04,  -4.85936616e-05],\n",
       "        [ -9.19141803e-06,  -7.82970674e-05,   8.74884790e-05],\n",
       "        [ -2.36922278e-05,  -2.38713747e-05,   4.75636043e-05]], dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the tf output of weight update and our own\n",
    "W1 - W0, delta_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01101298, -0.00845931, -0.00255367], dtype=float32),\n",
       " array([[ 0.01101298, -0.00845931, -0.00255367]], dtype=float32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the tf output of bias update and our own\n",
    "b1 - b0, delta_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-hidden layer ANN\n",
    "\n",
    "Now we add a single hidden layer to our softmax model.\n",
    "\n",
    "$$ h_\\alpha = \\sigma ( x_\\mu W_\\alpha^\\mu ) $$\n",
    "\n",
    "where $\\mu = 0,1,\\ldots ,4$ with $0$ indexing the bias, and $\\alpha$ indexes the hidden nodes with $0$ indicating the hidden bias. $\\sigma$ is the standard [logistic function](https://en.wikipedia.org/wiki/Logistic_function) aka sigmoid function and for this exercise we choose it as the *activation function*. Note the derivative property $\\sigma^\\prime = \\sigma (1-\\sigma)$ that we will use in deriving the backpropagation rule.\n",
    "\n",
    "Logit for the output layer is given by\n",
    "\n",
    "$$ \\eta_k = h_\\alpha W_k^\\alpha $$\n",
    "\n",
    "Probability prediction is\n",
    "\n",
    "$$ p_k = \\frac{e^{\\eta_k}}{Z} $$\n",
    "\n",
    "Loss function is (again) defined as\n",
    "\n",
    "$$ l = - \\Bigg \\langle \\sum_k y_k \\log p_k \\Bigg \\rangle $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement the 1-hidden layer ANN\n",
    "x = tf.placeholder(tf.float32,[None,4])\n",
    "# input -> hidden\n",
    "number_of_hidden_nodes = 5\n",
    "W1 = tf.Variable(tf.random_normal([4,number_of_hidden_nodes]),tf.float32)\n",
    "b1 = tf.Variable(tf.zeros([number_of_hidden_nodes]),tf.float32)\n",
    "eta1 =  tf.add(tf.matmul(x,W1),b1)\n",
    "h1 = tf.sigmoid(eta1)\n",
    "W2 = tf.Variable(tf.random_normal([number_of_hidden_nodes,3]),tf.float32)\n",
    "b2 = tf.Variable(tf.zeros([3]),tf.float32)\n",
    "eta2 = tf.add(tf.matmul(h1,W2),b2)\n",
    "prob = tf.nn.softmax(eta2)\n",
    "# Actual class\n",
    "y = tf.placeholder(tf.float32,[None,3])\n",
    "\n",
    "# Define the loss\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(prob),reduction_indices=[1]) / tf.log(3.)\n",
    "# normalize by log(3) just so the ``uninformative'' prob gives cross-entropy 1.\n",
    "loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = .05\n",
    "batch_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the optimization method\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "fit = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation)\n",
    "\n",
    "Take the 1st order differential of the loss function\n",
    "\n",
    "\\begin{equation}\n",
    "dl = - \\Bigg \\langle \\sum_k (y_k - p_k) d\\eta_k \\Bigg \\rangle\n",
    "\\end{equation}\n",
    "\n",
    "The gradient with respect to the output layer weight is of the same form as that for softmax regression except $x$ is replaced with $h$\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial W^\\alpha_k} = - \\Bigg\\langle h_\\alpha (y_k - p_k) \\Bigg\\rangle $$\n",
    "\n",
    "The gradient with respect to the hidden layer weight, however, is computed by [chain rule](https://en.wikipedia.org/wiki/Chain_rule) that is referred as *backpropagation* in this context. We spell out all the implicit indices and summation in the following expression\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial W^\\mu_\\alpha} = - {1\\over {N\\log 3}}\\sum_{i=1}^N x^i_\\mu \\sum_{k=1}^3(y^i_k - p^i_k) W^\\alpha_k (\\sigma(1-\\sigma))^i_\\alpha\n",
    "$$\n",
    "\n",
    "Note there is no summation over $\\alpha$ and we already used the property $\\sigma^\\prime = \\sigma (1-\\sigma)$. \n",
    "\n",
    "To implement this particular backpropagation, we first contract class index $k$, then carry out the element-wise multiplication over $\\alpha$, and then contract on sample index $i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient of loss /w respect to W2\n",
    "grad_W2 = -tf.matmul(tf.transpose(h1),(y-prob))/tf.log(3.)/batch_size\n",
    "# Gradient of loss /w respect to b2\n",
    "grad_b2 = -tf.matmul(tf.ones([1,batch_size]),(y-prob))/tf.log(3.)/batch_size\n",
    "# Gradient of loss /w respect to W1\n",
    "grad_W1 = -tf.matmul(tf.transpose(x),tf.matmul((y-prob), tf.transpose(W2)) * h1 * (1-h1)) /tf.log(3.)/batch_size\n",
    "# Gradient of loss /w respect to b1\n",
    "grad_b1 = -tf.matmul(tf.ones([1,batch_size]),tf.matmul((y-prob), tf.transpose(W2)) * h1 * (1-h1)) /tf.log(3.)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initiate the computation\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Record the weight before a single batch\n",
    "W1_0,b1_0,W2_0,b2_0 = sess.run([W1,b1,W2,b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get next mini batch\n",
    "btch_x,btch_y = training_dataset.next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight update for the readout layer\n",
    "delta_W2, delta_b2 = sess.run([-lr*grad_W2, -lr*grad_b2],{x:btch_x,y:btch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute weight update for the hidden layer\n",
    "delta_W1, delta_b1 = sess.run([-lr*grad_W1, -lr*grad_b1],{x:btch_x,y:btch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run one mini batch\n",
    "sess.run(fit,{x:btch_x,y:btch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tf output of the updated weight per a single batch\n",
    "W1_1,b1_1,W2_1,b2_1 = sess.run([W1,b1,W2,b2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following two Jupyter Notebook cells compare the tensorflow weight update and our computed weight update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00620022,  0.00803447, -0.01423478],\n",
       "        [ 0.00616717,  0.00799727, -0.01416445],\n",
       "        [ 0.00620443,  0.00799352, -0.01419795],\n",
       "        [ 0.00614983,  0.00798869, -0.01413846],\n",
       "        [ 0.0061354 ,  0.00793525, -0.01407063]], dtype=float32),\n",
       " array([[ 0.00620024,  0.00803446, -0.01423469],\n",
       "        [ 0.0061672 ,  0.00799728, -0.01416448],\n",
       "        [ 0.00620445,  0.00799354, -0.01419799],\n",
       "        [ 0.00614982,  0.00798868, -0.01413849],\n",
       "        [ 0.0061354 ,  0.00793525, -0.01407065]], dtype=float32),\n",
       " array([ 0.01231238,  0.01591982, -0.02823219], dtype=float32),\n",
       " array([[ 0.01231237,  0.01591982, -0.02823219]], dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the tf weight update and hand-coded weight update in the hidden layer\n",
    "W2_1 - W2_0, delta_W2, b2_1 - b2_0, delta_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -6.33656979e-04,  -7.88271427e-06,   3.25709581e-04,\n",
       "          -2.26378441e-04,  -9.64403152e-05],\n",
       "        [ -3.31297517e-04,   2.27801502e-06,   1.75267458e-04,\n",
       "          -1.52826309e-04,  -5.31673431e-05],\n",
       "        [ -3.88268381e-04,  -1.64359808e-05,   1.90556049e-04,\n",
       "          -7.61747360e-05,  -5.41210175e-05],\n",
       "        [ -1.20155513e-04,  -6.10947609e-06,   5.81741333e-05,\n",
       "          -1.80602074e-05,  -1.63167715e-05]], dtype=float32),\n",
       " array([[ -6.33633055e-04,  -7.88942907e-06,   3.25695641e-04,\n",
       "          -2.26319258e-04,  -9.64305436e-05],\n",
       "        [ -3.31294228e-04,   2.27736905e-06,   1.75264402e-04,\n",
       "          -1.52810855e-04,  -5.31562619e-05],\n",
       "        [ -3.88267857e-04,  -1.64383364e-05,   1.90557694e-04,\n",
       "          -7.61863485e-05,  -5.41271475e-05],\n",
       "        [ -1.20155353e-04,  -6.11466839e-06,   5.81723252e-05,\n",
       "          -1.80428233e-05,  -1.63110180e-05]], dtype=float32),\n",
       " array([-0.02670146, -0.00016045,  0.01385854, -0.01046353, -0.00413716], dtype=float32),\n",
       " array([[-0.02670146, -0.00016045,  0.01385854, -0.01046353, -0.00413716]], dtype=float32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the tf weight update and hand-coded weight update in the hidden layer\n",
    "W1_1 - W1_0, delta_W1, b1_1 - b1_0, delta_b1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
